import torch
import torch.nn as nn
from torch.nn import functional as F

class RegressionModel(nn.Module):

    def __init__(self, with_clinical=False, middle=2, clinical_ = 4) -> None:
        super().__init__()
        self.with_clinical = with_clinical
        self.encoder_model = UNet3DEncoder(4, 3)
        self.flatten = nn.Flatten()
        if self.with_clinical:
            clinical_list = [nn.Linear(4, 8)]
            for i in range(3):
                clinical_list += [
                    nn.SELU(), nn.AlphaDropout(),
                    nn.Linear(8, 8)
                ]
            clinical_list.append(nn.Linear(8, clinical_))
            self.clinical_model = nn.Sequential(*clinical_list)
        seq_list = [
            nn.Linear(512 * 8 * 8 * 8, 512),
            nn.Dropout(0.1),
            nn.ReLU(),
            nn.Linear(512, 64),
            nn.Dropout(0.1),
            nn.ReLU(),
            nn.Linear(64, 8),
            nn.Dropout(0.1),
            nn.ReLU(),
            nn.Linear(8, 1)
        ]

        self.decoder_model = nn.Sequential(*seq_list[:3*middle])
        #combine_layer: nn.Linear = seq_list[3*middle]
        if with_clinical:
            combine_layer = nn.Linear(seq_list[3*middle].in_features + (clinical_ if with_clinical else 0), seq_list[3*middle].out_features)
            self.output_model = nn.Sequential(combine_layer, *seq_list[3*middle+1:])
        else:
            self.output_model = nn.Sequential(*seq_list[3*middle:])

    def forward(self, x, age, resection):
        # embedding the input
        x = self.encoder_model(x)
        # # unpacking the embedded features generated by the transformer
        x = self.flatten(x[3])
        # decoding the embedded features
        x = self.decoder_model(x)
        if self.with_clinical:
            #clinical = self.clinical_encoder(torch.cat((age, resection), dim=1))
            clinical_data = self.clinical_model(
                torch.cat((age, resection), dim=1))
            x = torch.cat((x, clinical_data), dim=1)
        x = self.output_model(x)
        return x

class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels, bath_normal=False):
        super(DoubleConv, self).__init__()
        channels = out_channels // 2
        if in_channels > out_channels:
            channels = in_channels // 2

        layers = [
            # in_channels：输入通道数
            # channels：输出通道数
            # kernel_size：卷积核大小
            # stride：步长
            # padding：边缘填充

            nn.Conv3d(in_channels, channels, kernel_size=3, stride=1, padding=0),
            nn.ReLU(True),

            nn.Conv3d(channels, out_channels, kernel_size=3, stride=1, padding=0),
            nn.ReLU(True)
        ]
        if bath_normal: # 如果要添加BN层
            layers.insert(1, nn.BatchNorm3d(channels))
            layers.insert(len(layers) - 1, nn.BatchNorm3d(out_channels))

        # 构造序列器
        self.double_conv = nn.Sequential(*layers)

    def forward(self, x):
        return self.double_conv(x)

class DownSampling(nn.Module):
    def __init__(self, in_channels, out_channels, batch_normal=False):
        super(DownSampling, self).__init__()
        self.maxpool_to_conv = nn.Sequential(
            nn.MaxPool3d(kernel_size=2, stride=2),
            DoubleConv(in_channels, out_channels, batch_normal)
        )

    def forward(self, x):
        return self.maxpool_to_conv(x)

class UpSampling(nn.Module):
    def __init__(self, in_channels, out_channels, batch_normal=False, bilinear=True):
        super(UpSampling, self).__init__()
        if bilinear:
            # 采用双线性插值的方法进行上采样
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        else:
            # 采用反卷积进行上采样
            self.up = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)
        self.conv = DoubleConv(in_channels + in_channels / 2, out_channels, batch_normal)

    # inputs1：上采样的数据（对应图中黄色箭头传来的数据）
    # inputs2：特征融合的数据（对应图中绿色箭头传来的数据）
    def forward(self, inputs1, inputs2):
        # 进行一次up操作
        inputs1 = self.up(inputs1)

        # 进行特征融合
        outputs = torch.cat([inputs1, inputs2], dim=1)
        outputs = self.conv(outputs)
        return outputs

class LastConv(nn.Module):
    def __init__(self, in_channels, out_channels ):
        super(LastConv, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1 )

    def forward(self, x):
        return self.conv(x)

class UNet3DEncoderSmall(nn.Module):
    def __init__(self, in_channels, num_classes=3, batch_normal=False, bilinear=True):
        super().__init__()
        self.in_channels = in_channels
        self.batch_normal = batch_normal
        self.bilinear = bilinear

        self.inputs = DoubleConv(in_channels, 32, self.batch_normal)
        self.down_1 = DownSampling(32, 64, self.batch_normal)
        self.down_2 = DownSampling(64, 160, self.batch_normal)
        self.down_3 = DownSampling(160, 256, self.batch_normal)

    def forward(self, x):
        # down 部分
        x1 = self.inputs(x)
        x2 = self.down_1(x1)
        x3 = self.down_2(x2)
        x4 = self.down_3(x3)
        #print(x1.shape, x2.shape, x3)
        return x1, x2, x3, x4

class UNet3DEncoder(nn.Module):
    def __init__(self, in_channels, num_classes=3, batch_normal=False, bilinear=True):
        super().__init__()
        self.in_channels = in_channels
        self.batch_normal = batch_normal
        self.bilinear = bilinear

        self.inputs = DoubleConv(in_channels, 64, self.batch_normal)
        self.down_1 = DownSampling(64, 128, self.batch_normal)
        self.down_2 = DownSampling(128, 256, self.batch_normal)
        self.down_3 = DownSampling(256, 512, self.batch_normal)

    def forward(self, x):
        # down 部分
        x1 = self.inputs(x)
        x2 = self.down_1(x1)
        x3 = self.down_2(x2)
        x4 = self.down_3(x3)

        return x1, x2, x3, x4

class UNet3DDecoder(nn.Module):
    def __init__(self, in_channels, num_classes=3, batch_normal=False, bilinear=True):
        super().__init__()
        self.in_channels = in_channels
        self.batch_normal = batch_normal
        self.bilinear = bilinear

        self.up_1 = UpSampling(512, 256, self.batch_normal, self.bilinear)
        self.up_2 = UpSampling(256, 128, self.batch_normal, self.bilinear)
        self.up_3 = UpSampling(128, 64, self.batch_normal, self.bilinear)
        self.outputs = LastConv(64, num_classes)

    def forward(self, x1, x2, x3, x4):
        # down 部分

        # up部分
        x5 = self.up_1(x4, x3)
        x6 = self.up_2(x5, x2)
        x7 = self.up_3(x6, x1)
        x = self.outputs(x7)

        return x

class UNet3D(nn.Module):
    def __init__(self, in_channels, num_classes=2, batch_normal=False, bilinear=True):
        super(UNet3D, self).__init__()
        self.in_channels = in_channels
        self.batch_normal = batch_normal
        self.bilinear = bilinear

        self.inputs = DoubleConv(in_channels, 64, self.batch_normal)
        self.down_1 = DownSampling(64, 128, self.batch_normal)
        self.down_2 = DownSampling(128, 256, self.batch_normal)
        self.down_3 = DownSampling(256, 512, self.batch_normal)

        self.up_1 = UpSampling(512, 256, self.batch_normal, self.bilinear)
        self.up_2 = UpSampling(256, 128, self.batch_normal, self.bilinear)
        self.up_3 = UpSampling(128, 64, self.batch_normal, self.bilinear)
        self.outputs = LastConv(64, num_classes)

    def forward(self, x):
        # down 部分
        x1 = self.inputs(x)
        x2 = self.down_1(x1)
        x3 = self.down_2(x2)
        x4 = self.down_3(x3)

        # up部分
        x5 = self.up_1(x4, x3)
        x6 = self.up_2(x5, x2)
        x7 = self.up_3(x6, x1)
        x = self.outputs(x7)

        return x

if __name__ == "__main__":
    input = torch.randint(
        low=0,
        high=255,
        size=(1, 4, 128, 128, 128),
        dtype=torch.float,
    )
    input = input.to("cuda:0")
    segformer3D = UNet3DEncoder(4).to("cuda:0")
    output = segformer3D(input)
    print(output[-1].shape)
